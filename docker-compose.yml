services:
  backend:
    image: unmute-backend:latest
    build:
      context: ./
      target: hot-reloading
    volumes:
      - ./unmute:/app/unmute
    environment:
      - KYUTAI_STT_URL=ws://stt:8080
      - KYUTAI_TTS_URL=ws://tts:8080
      # Ollama configuration
      - KYUTAI_LLM_URL=http://host.docker.internal:11434
      - KYUTAI_LLM_MODEL=llama3.2
      - KYUTAI_LLM_API_KEY=ollama
      - NEWSAPI_API_KEY=$NEWSAPI_API_KEY
    extra_hosts:
      - "host.docker.internal:host-gateway"
    ports:
      - "8000:80"
    depends_on:
      - stt
      - tts

  tts:
    image: moshi-server:latest
    command: ["worker", "--config", "configs/tts.toml"]
    build:
      context: services/moshi-server
      dockerfile: public.Dockerfile
    environment:
      - HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN
    volumes:
      - ./volumes/hf-cache:/root/.cache/huggingface
      - ./volumes/cargo-registry-tts:/root/.cargo/registry
      - ./volumes/tts-target:/app/target
      - ./volumes/uv-cache:/root/.cache/uv
      - /tmp/models/:/models
      - ./volumes/tts-logs:/tmp/unmute_logs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  stt:
    image: moshi-server:latest
    command: ["worker", "--config", "configs/stt.toml"]
    build:
      context: services/moshi-server
      dockerfile: public.Dockerfile
    environment:
      - HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN
    volumes:
      - ./volumes/hf-cache:/root/.cache/huggingface
      - ./volumes/cargo-registry-stt:/root/.cargo/registry
      - ./volumes/stt-target:/app/target
      - ./volumes/uv-cache:/root/.cache/uv
      - /tmp/models/:/models
      - ./volumes/stt-logs:/tmp/unmute_logs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

networks:
  default:
