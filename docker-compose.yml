services:
  backend:
    image: unmute-backend:latest
    build:
      context: ./
      target: hot-reloading
    volumes:
      - ./unmute:/app/unmute
    network_mode: host
    environment:
      - KYUTAI_STT_URL=ws://localhost:8088
      - KYUTAI_TTS_URL=ws://localhost:8089
      # Remote Ollama via Tailscale
      - KYUTAI_LLM_URL=http://100.126.172.56:11434
      - KYUTAI_LLM_MODEL=ministral-3:14b
      - KYUTAI_LLM_API_KEY=ollama
      - NEWSAPI_API_KEY=$NEWSAPI_API_KEY
    depends_on:
      - stt
      - tts

  tts:
    image: moshi-server:latest
    command: ["worker", "--config", "configs/tts.toml"]
    build:
      context: services/moshi-server
      dockerfile: public.Dockerfile
    environment:
      - HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN
    ports:
      - "8089:8080"
    volumes:
      - ./volumes/hf-cache:/root/.cache/huggingface
      - ./volumes/cargo-registry-tts:/root/.cargo/registry
      - ./volumes/tts-target:/app/target
      - ./volumes/uv-cache:/root/.cache/uv
      - /tmp/models/:/models
      - ./volumes/tts-logs:/tmp/unmute_logs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  stt:
    image: moshi-server:latest
    command: ["worker", "--config", "configs/stt.toml"]
    build:
      context: services/moshi-server
      dockerfile: public.Dockerfile
    environment:
      - HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN
    ports:
      - "8088:8080"
    volumes:
      - ./volumes/hf-cache:/root/.cache/huggingface
      - ./volumes/cargo-registry-stt:/root/.cargo/registry
      - ./volumes/stt-target:/app/target
      - ./volumes/uv-cache:/root/.cache/uv
      - /tmp/models/:/models
      - ./volumes/stt-logs:/tmp/unmute_logs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

networks:
  default:
